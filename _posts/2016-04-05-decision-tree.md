---
layout: post
title: Decision Tree
subtitle: ''
date: 2016-04-05T00:00:00.000Z
author: Run.D.Guan
header-img: img/post-macha.jpg
category: Machine-learning
tags:
  - Supervised-Learning
  - Classification
---

### Preface
决策树是我认识中最符合人直观思维的机器学习算法了，当我们判断一个问题时，我们就会寻求一些criteria一步步来分析该问题，最终得出结论。比如对于要不要出去玩网球如此"纠结"的问题，我们可以考虑如下的决定因素：天气看起来怎么样、温度怎么样、湿度如何以及有没有风（这个例子中显然缺少一个关键因素--有没有写完作业啊，果然老外不太考虑这些）。那么思考的过程可以形成如下的一棵决策树

![tree](http://7xqutp.com1.z0.glb.clouddn.com/dt2.PNG)

很符合自然的思考过程吧，下面描述下该决策树：

 * 每个结点测试一个属性（如天气、温度啊）
 * 结点的每条分支都是从该属性选取的值（对于天气结点就是Sunny、Overcast和Rain了）
 * 每个叶子结点就是预测的输出 $C_i$

决策树学习的目的是为了产生一棵泛化能力强，即处理未见示例能力强的决策树，其基本流程遵循如下递归策略

>**输入：** 训练集 $D=\\{(x_1,y_1),(x_2,y_2),\dots, (x_m,y_m)\\}$ ，属性值 $A=\\{a_1,a_2,\dots,a_d\\}$  
设 $D_t$ 是与结点 $t$ 相关联的训练集
>1.  **if** $D_t$ 中样本都属于同一类 $C_i$，  **then** 将 $t$ 标记为叶子结点；**return**  
>2.  **if** $A=\varnothing$， **then** 将 $t$ 标记为叶子结点；其类别标记为 $D_t$ 中样本数最多的类；**return**  
>3.  从 $A$ 选取一个最优属性测试条件，将样本划分为较小的子集。对测试条件的每个输出创建一个子结点，并根据测试结果将样本分布到子结点中。 **then** 对每个子节点递归调用该算法。

无论哪种决策树算法，流程基本都是上述的过程，关键就在于如何选取所谓的最优属性测试条件。决策树构建算法主要有**ID3**，**C4.5**和**CART**。

### 划分选择

决策树的学习过程，我们希望每次划分出的分支尽可能属于同一类别，即结点的“纯度”越来越高。考察下面以哪种属性作为分类条件更好呢？

![](http://7xqutp.com1.z0.glb.clouddn.com/attri.png?imageView/2/w/500/q/100)

#### 信息增益
信息熵（information entropy）是度量样本集合纯度最常用的一种指标，样本集合纯度越低（即系统越混乱），信息熵越高，反之则低。假定当前样本集合 $D$ 中第 $i$ 类样本所占的比例为 $p_i (i=1,2,\dots,n)$，则 $D$ 的信息熵定义为

$$
    H(D)=-\sum_{i=1}^{n}p_i\log_2p_i
$$

编码样本集合 $i$ 需要分配的最少二进制位数为 $-\log_2p_i$，所以 $H(D)$ 描述了从 $D$ 中随机选取样本编码的位数的期望。数假设只分为两类(+,-)，熵的$1/2$ 随+类概率 $P_+$ 变化如下图，

![](http://7xqutp.com1.z0.glb.clouddn.com/dt1.PNG?imageView/2/w/410/q/90)

基尼系数后面会介绍，而分类误差率为 $1-\max(p_+,1-p_+)$，而且它总不会大于0.5，因为如果本来猜想的概率低，那么反着猜就好啦。
